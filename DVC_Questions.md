# ðŸš€ **What is DVC (in simple words)**

DVC is **Git for data + ML pipelines**.

It helps you version:

* datasets
* preprocessing outputs
* model files
* ML pipelines

â€¦without putting large files directly in GitHub.

You get reproducibility **without polluting your repo**.

---

# ðŸ”§ Why DVC is important in MLOps

### âœ” Decouples large files from Git

Git canâ€™t handle big datasets.
DVC stores them **in remote storage** (Google Drive, S3, local folder, etc.).

### âœ” Ensures reproducibility

Run the same experiment months later â†’ get the **exact same data** and **pipeline output**.

### âœ” Works perfectly with MLflow

MLflow tracks:

* parameters
* metrics
* model versions

DVC tracks:

* data
* intermediate artifacts
* pipeline dependencies

---

# ðŸ§­ Part 1 â€” Install DVC

Pick storage backend:

* If you want something easy: **Google Drive**
* If offline: **local folder**

Install:

```bash
pip install dvc
```

Optionally (Google Drive):

```bash
pip install dvc[gdrive]
```

---

# ðŸ§­ Part 2 â€” Initialize DVC

Inside your ML project folder:

```bash
dvc init
```

This creates:

* `.dvc/` directory
* `.dvcignore`
* hooks for Git integration

Add these to Git:

```bash
git add .dvc .dvcignore
git commit -m "Initialize DVC"
```

---

# ðŸ§­ Part 3 â€” Track your dataset

Example directory:

```
data/
   train.csv
   test.csv
```

Track it with DVC:

```bash
dvc add data/train.csv
```

This creates:

* `data/train.csv.dvc` (small metadata file)
* DVC moves the actual data into its cache

Commit the metadata to Git:

```bash
git add data/train.csv.dvc .gitignore
git commit -m "Track training data with DVC"
```

**Important:**
Your GitHub repo *never sees the actual CSV file*.
Only the `.dvc` metadata is stored.

---

# ðŸ§­ Part 4 â€” Add remote storage

Local remote (easy):

```bash
dvc remote add -d myremote /mnt/dvc-storage
```

Google Drive remote:

```bash
dvc remote add -d myremote gdrive://<folder-id>
```

Push your data:

```bash
dvc push
```

Now your dataset is safely stored in remote storage.

---

# ðŸ§­ Part 5 â€” Reproduce pipelines (DVC magic)

Create a training script:

```
src/train.py
```

Register a pipeline stage:

```bash
dvc stage add -n train \
  -d src/train.py -d data/train.csv \
  -o model.pkl \
  python src/train.py
```

This creates `dvc.yaml` with:

```yaml
stages:
  train:
    cmd: python src/train.py
    deps:
    - src/train.py
    - data/train.csv
    outs:
    - model.pkl
```

Run the pipeline:

```bash
dvc repro
```

This is **DVCâ€™s most powerful feature** â†’ it rebuilds only what changed.

---

# ðŸ§­ Part 6 â€” Working with teammates

Clone repo:

```bash
git clone <repo>
```

Get the data:

```bash
dvc pull
```

Now you have:

* same data
* same scripts
* same pipeline outputs

Repro guarantees **identical execution**.

---

# ðŸ§  How DVC + MLflow fit together

| Need                    | Tool             |
| ----------------------- | ---------------- |
| Track model metrics     | MLflow           |
| Track model artifacts   | MLflow           |
| Track model versions    | MLflow           |
| Track data versions     | DVC              |
| Track pipeline stages   | DVC              |
| Reproduce full training | DVC              |
| Deploy model            | FastAPI / Docker |
| Automate                | Airflow          |

They complement each other, not replace.

---
